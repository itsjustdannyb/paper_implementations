{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ca61c5",
   "metadata": {},
   "source": [
    "## Training data-efficient image transformers & distillation through attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7b472",
   "metadata": {},
   "source": [
    "Vision Transformers (ViT) are data hungry, to work well they were trained on a private dataset with 300 Images (JFT-300M) then finetuned for a different doenstream task.\n",
    "\n",
    "Data Efficient Image Transformers solves this problem by using **Knowledge Distillation** from another architecture (teacher) to a Transformer based Architecture (student) while using only one dataset (Imagenet)\n",
    "\n",
    "From the paper; page 2:\n",
    "\n",
    "In summary, our work makes the following contributions:\n",
    "* We show that our neural networks that contains no convolutional layer\n",
    "can achieve competitive results against the state of the art on ImageNet\n",
    "with no external data. They are learned on a single node with 4 GPUs in\n",
    "three days. Our two new models DeiT-S and DeiT-Ti have fewer parameters\n",
    "and can be seen as the counterpart of ResNet-50 and ResNet-18.\n",
    "* We introduce a new distillation procedure based on a distillation token,\n",
    "which plays the same role as the class token, except that it aims at reproducing\n",
    "the label estimated by the teacher. Both tokens interact in the\n",
    "transformer through attention. This transformer-specific strategy outperforms\n",
    "vanilla distillation by a significant margin.\n",
    "* Interestingly, with our distillation, image transformers learn more from a\n",
    "convnet than from another transformer with comparable performance.\n",
    "* Our models pre-learned on Imagenet are competitive when transferred to\n",
    "different downstream tasks such as fine-grained classification, on several\n",
    "popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\n",
    "Stanford Cars and iNaturalist-18/19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40014eaa",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "\n",
    "Knowledge Distillation (KD), introduced by Hinton et al. [24], refers to the\n",
    "training paradigm in which a student model leverages “soft” labels coming\n",
    "from a strong teacher network. This is the output vector of the teacher’s softmax\n",
    "function rather than just the maximum of scores, wich gives a “hard” label.\n",
    "Such a training improves the performance of the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa08df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5e418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff82df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931e732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
